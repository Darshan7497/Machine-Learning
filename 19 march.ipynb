{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fb9cc3-c438-4f0a-a266-a589bc395b95",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925116b-0876-4262-a270-8bfdde19acb4",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numerical features to a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to bring all the features on a similar scale, preventing any particular feature from dominating the analysis due to its larger value range.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "Here's an example to illustrate its application:\n",
    "\n",
    "Suppose we have a dataset with a feature \"Age\" representing the age of individuals. The minimum age is 20, and the maximum age is 60. We want to scale this feature using Min-Max scaling.\n",
    "\n",
    "Original values of \"Age\":\n",
    "[25, 30, 40, 50, 55]\n",
    "\n",
    "To apply Min-Max scaling, we calculate the scaled values using the formula:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "Scaled values of \"Age\":\n",
    "[(25-20) / (60-20), (30-20) / (60-20), (40-20) / (60-20), (50-20) / (60-20), (55-20) / (60-20)]\n",
    "\n",
    "Simplified scaled values:\n",
    "[0.125, 0.25, 0.5, 0.75, 0.875]\n",
    "\n",
    "As a result, the \"Age\" values are now scaled between 0 and 1, with the minimum value transformed to 0 and the maximum value transformed to 1.\n",
    "\n",
    "Min-Max scaling is useful when the absolute values of the features are not as important as their relative positions or when the features have different scales. It helps to prevent bias towards features with larger value ranges and can be beneficial for algorithms that are sensitive to the scale of the input data, such as gradient descent-based optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393c4ce-9cac-4cbc-867a-05d86f4b127d",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff23626-b4ae-4818-8d97-a68563bf51f2",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or feature normalization, is a feature scaling technique that scales the values of a feature to have a unit norm. Unlike Min-Max scaling, which scales the values to a specific range, the Unit Vector technique focuses on the direction or orientation of the feature vector rather than its magnitude.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "unit_vector = value / ||value||\n",
    "\n",
    "Here, ||value|| represents the Euclidean norm or magnitude of the vector.\n",
    "\n",
    "The Unit Vector technique ensures that all the feature vectors have a length or magnitude of 1, effectively placing them on the surface of a unit hypersphere. It is particularly useful when the direction of the feature vector is more important than its magnitude.\n",
    "\n",
    "Let's illustrate its application with an example:\n",
    "\n",
    "Suppose we have a dataset with a feature \"Length\" representing the lengths of objects. The original values are as follows:\n",
    "\n",
    "[2, 4, 6, 8, 10]\n",
    "\n",
    "To apply the Unit Vector technique, we calculate the unit vectors using the formula:\n",
    "\n",
    "unit_vector = value / ||value||\n",
    "\n",
    "Unit vectors of \"Length\":\n",
    "[2 / ||2||, 4 / ||4||, 6 / ||6||, 8 / ||8||, 10 / ||10||]\n",
    "\n",
    "Simplified unit vectors:\n",
    "[1 / 1, 1 / 1, 1 / 1, 1 / 1, 1 / 1]\n",
    "\n",
    "As a result, the \"Length\" values are transformed into unit vectors, where each vector has a length of 1.\n",
    "\n",
    "The Unit Vector technique is commonly used in machine learning algorithms that are sensitive to the scale or magnitude of the input features but not their absolute values. It helps to normalize the features' directions and can be beneficial in scenarios where the relative orientations of the features are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319d80b-1144-4548-af08-8dfdbfa5ee78",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c82fd8-4be5-41bf-9b2f-0fc31bd92369",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while retaining the most important information. It achieves this by identifying the principal components, which are orthogonal directions that capture the maximum variance in the data.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Suppose we have a dataset with three features: \"Length,\" \"Width,\" and \"Height.\" The goal is to reduce the dimensionality of the dataset while preserving the most relevant information.\n",
    "\n",
    "Standardize the data: Before applying PCA, it is recommended to standardize the features to have zero mean and unit variance. This step ensures that features with larger scales do not dominate the analysis.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships and variances between the features.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: Determine the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each component.\n",
    "\n",
    "Select the desired number of principal components: Choose the number of principal components to retain based on the desired level of dimensionality reduction. Typically, the components are ranked in descending order of their eigenvalues.\n",
    "\n",
    "Project the data onto the selected components: Project the standardized data onto the selected principal components to obtain the reduced-dimensional representation of the dataset.\n",
    "\n",
    "By using PCA, we can reduce the dimensionality of the dataset while retaining the most significant information. The reduced dataset will have fewer dimensions (typically fewer than the original features) but will still capture a significant portion of the variance in the data.\n",
    "\n",
    "PCA finds application in various fields, such as image processing, genetics, finance, and pattern recognition, where high-dimensional data can be effectively represented in a lower-dimensional space. It helps in visualization, noise reduction, and improving computational efficiency in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1509571-c4dd-491b-9bf3-1daaef29b620",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee7bf2-7ed4-4053-bf87-74cfeec9a9ac",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) can be used as a feature extraction technique. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important information.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA identifies the principal components, which are the linear combinations of the original features. These principal components capture the maximum variance in the data and can be considered as new features.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with 1000 samples and 50 features. We want to reduce the dimensionality of the dataset by extracting the most important features.\n",
    "\n",
    "Standardize the data: Before applying PCA, it is recommended to standardize the features to have zero mean and unit variance.\n",
    "\n",
    "Apply PCA: Compute the covariance matrix of the standardized data and find the eigenvectors and eigenvalues. Sort the eigenvectors based on their corresponding eigenvalues in descending order.\n",
    "\n",
    "Select the desired number of principal components: Choose the number of principal components based on the desired level of dimensionality reduction. The number can be determined by considering the amount of variance explained by each principal component.\n",
    "\n",
    "Project the data onto the selected components: Use the selected principal components to project the standardized data onto the new feature space. This transformation yields a reduced-dimensional representation of the dataset.\n",
    "\n",
    "The resulting transformed dataset will have a reduced number of features, corresponding to the selected principal components. These new features are linear combinations of the original features and capture the most significant information in the data.\n",
    "\n",
    "PCA-based feature extraction is particularly useful when dealing with high-dimensional datasets, as it can help in reducing noise, removing redundancy, and improving computational efficiency. The extracted features can then be used as input for various machine learning algorithms or further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034a7e5-5da9-4202-979b-2c4dd8f45ae1",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f08daf31-1039-462b-acbb-0406da3aee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "         'Price' :[10,12,15,20,25],\n",
    "         'Rating':[5,4.5,3,2,5],\n",
    "         'Delivery_time' :[10,15,30,25,9]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7208169-0c6d-4698-ada6-5753da1617f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45807790-1909-4917-ab64-2e00fc887b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Delivery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>4.5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price  Rating  Delivery_time\n",
       "0     10     5.0             10\n",
       "1     12     4.5             15\n",
       "2     15     3.0             30\n",
       "3     20     2.0             25\n",
       "4     25     5.0              9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de407b8-3cd8-4ad0-8269-ac500061b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6a1e67d-4f86-4d29-a5e8-9526c13a6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max =MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d15109b-4f27-4133-90d2-bcd3950d6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = min_max.fit_transform(df[['Price','Rating','Delivery_time']])\n",
    "df_scaled =pd.DataFrame(df1 , columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da631ac-1374-421c-9362-5cb2af2d77e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Delivery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price    Rating  Delivery_time\n",
       "0  0.000000  1.000000       0.047619\n",
       "1  0.133333  0.833333       0.285714\n",
       "2  0.333333  0.333333       1.000000\n",
       "3  0.666667  0.000000       0.761905\n",
       "4  1.000000  1.000000       0.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c69702-13e8-424d-ae86-fff9822fc19c",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b04ade-093a-4970-a84b-fcec71d0877b",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices with a dataset containing many features, such as company financial data and market trends, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset. Here's an explanation of how PCA can be applied in this scenario:\n",
    "\n",
    "Standardize the data: Before applying PCA, it's important to standardize the data to ensure that each feature has a mean of 0 and a standard deviation of 1. This step is necessary because PCA is sensitive to the scale of the features.\n",
    "\n",
    "Apply PCA: Once the data is standardized, PCA can be applied to identify the principal components. The principal components are new orthogonal variables that are a linear combination of the original features. The first principal component explains the maximum amount of variance in the data, and each subsequent component explains the remaining variance in decreasing order.\n",
    "\n",
    "Determine the number of components: To decide on the number of components to retain, we can analyze the explained variance ratio. The explained variance ratio for each component indicates the proportion of variance explained by that component. By choosing a cumulative explained variance threshold (e.g., 95%), we can determine the minimum number of components needed to capture that threshold.\n",
    "\n",
    "Project the data onto the selected components: Once the desired number of components is determined, the original data can be projected onto these components. This projection results in a reduced-dimensional representation of the data, where each sample is represented by the values along the selected components.\n",
    "\n",
    "By using PCA to reduce the dimensionality of the dataset, we can achieve several benefits:\n",
    "\n",
    "Reducing the number of features simplifies the model and reduces computational complexity.\n",
    "The new set of orthogonal components can potentially capture the most important information in the data.\n",
    "It can help mitigate the curse of dimensionality by eliminating noise and redundant features.\n",
    "Visualization of the data becomes easier when plotting along the principal components.\n",
    "However, it's important to note that PCA may result in some information loss due to the reduction in dimensionality. The trade-off between dimensionality reduction and the preservation of information needs to be carefully considered based on the specific project requirements and the importance of interpretability.\n",
    "\n",
    "By applying PCA, we can effectively reduce the dimensionality of the dataset while retaining the most significant information, enabling us to build a more efficient and interpretable model for predicting stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4bbc54-c91e-4bc5-9a68-105a9a484963",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ed96f-dd42-4aa5-b390-75b850bf7204",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on a dataset and transform the values to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "Identify the minimum and maximum values in the dataset.\n",
    "\n",
    "Minimum value: 1\n",
    "Maximum value: 20\n",
    "Define the desired range for scaling. In this case, the range is -1 to 1.\n",
    "\n",
    "Apply the Min-Max scaling formula to each value in the dataset:\n",
    "\n",
    "scaled_value = (value - minimum) / (maximum - minimum) * (new_max - new_min) + new_min\n",
    "For our example, the formula becomes:\n",
    "\n",
    "scaled_value = (value - 1) / (20 - 1) * (1 - (-1)) + (-1)\n",
    "Calculate the scaled values for each element in the dataset using the formula:\n",
    "\n",
    "scaled_values = [((1 - 1) / (20 - 1) * (1 - (-1)) + (-1)),\n",
    "((5 - 1) / (20 - 1) * (1 - (-1)) + (-1)),\n",
    "((10 - 1) / (20 - 1) * (1 - (-1)) + (-1)),\n",
    "((15 - 1) / (20 - 1) * (1 - (-1)) + (-1)),\n",
    "((20 - 1) / (20 - 1) * (1 - (-1)) + (-1))]\n",
    "Calculate the scaled values:\n",
    "\n",
    "scaled_values = [ -1.0000, -0.5000, 0.0000, 0.5000, 1.0000]\n",
    "The resulting scaled values range from -1 to 1, preserving the relative proportions of the original dataset.\n",
    "\n",
    "Therefore, after applying Min-Max scaling to the dataset [1, 5, 10, 15, 20], the transformed values in the range of -1 to 1 are:\n",
    "[-1.0000, -0.5000, 0.0000, 0.5000, 1.0000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "User\n",
    "convert these theory into code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e0a8b96-ba77-484a-8519-fd270fb3b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the minimum and maximum values in the dataset\n",
    "minimum = np.min(data)\n",
    "maximum = np.max(data)\n",
    "\n",
    "# Define the desired range for scaling\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = (data - minimum) / (maximum - minimum) * (new_max - new_min) + new_min\n",
    "\n",
    "# Print the scaled values\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46f75b-bee2-410b-bf28-944dfbbaf863",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5e432-b8b8-4ad0-8493-01df61ad3883",
   "metadata": {},
   "source": [
    "To determine the number of principal components to retain in Feature Extraction using PCA, you can consider the cumulative explained variance ratio. The cumulative explained variance ratio represents the amount of information (variance) captured by each principal component and allows you to assess the proportion of total variance explained by a certain number of components.\n",
    "\n",
    "Here's an example of how you can perform Feature Extraction using PCA and decide the number of principal components to retain based on the explained variance ratio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "688e53fc-99cf-416e-af47-d0c952986dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed dataset:\n",
      "[[ -7.71655548   1.88337855]\n",
      " [  0.3243844    5.12342977]\n",
      " [-14.44635855  -3.96856143]\n",
      " [  6.6050192   -0.21129288]\n",
      " [ 15.23351043  -2.82695401]]\n",
      "Number of principal components selected: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the dataset with features [height, weight, age, gender, blood pressure]\n",
    "dataset = [[160, 60, 25, 1, 120],\n",
    "           [165, 68, 28, 0, 118],\n",
    "           [155, 55, 32, 0, 122],\n",
    "           [170, 70, 30, 1, 124],\n",
    "           [175, 75, 27, 1, 130]]\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(dataset)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Apply PCA with the selected number of components\n",
    "pca = PCA(n_components=num_components)\n",
    "transformed_dataset = pca.fit_transform(dataset)\n",
    "\n",
    "# Print the transformed dataset and the selected number of components\n",
    "print(\"Transformed dataset:\")\n",
    "print(transformed_dataset)\n",
    "print(\"Number of principal components selected:\", num_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f141343-b8c2-403a-8a7f-b0ca8cde95f3",
   "metadata": {},
   "source": [
    "In this example, the code applies PCA to the dataset and calculates the cumulative explained variance ratio. The number of principal components to retain is determined by finding the index where the cumulative variance ratio crosses a desired threshold, which in this case is 0.95. The selected number of components is then used to perform PCA again and transform the dataset.\n",
    "\n",
    "Note that the threshold of 0.95 is just an example. You can choose a different threshold based on your specific requirements and the amount of variance you want to retain in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
